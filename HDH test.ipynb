{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fc650500",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51c72c55",
   "metadata": {},
   "source": [
    "# Chargement des fichiers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "72c3aa64",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   id                         NUM_ENQ  prs_nat_ref exe_soi_dtd exe_soi_dtf  \\\n",
      "0   1  DPXX:000000000000000000000001X         1130  04/03/2013  04/03/2013   \n",
      "1   2  DPXX:000000000000000000000001X         1331  05/03/2013  05/03/2013   \n",
      "2   3  DPXX:000000000000000000000001X         3313  05/03/2013  05/03/2013   \n",
      "3   4  DPXX:000000000000000000000001X         3125  07/03/2013  07/03/2013   \n",
      "4   5  DPXX:000000000000000000000002X         1130  05/03/2013  01/07/2013   \n",
      "\n",
      "   pse_spe_cod  pse_act_nat  etb_pre_fin  \n",
      "0          1.0          NaN          NaN  \n",
      "1          6.0          NaN  750300360.0  \n",
      "2          NaN         50.0  750023772.0  \n",
      "3          NaN         26.0          NaN  \n",
      "4          6.0         26.0  750023772.0  \n"
     ]
    }
   ],
   "source": [
    "# Table contenant les informations sur les prestations remboursées\n",
    "er_prs_f=pd.read_csv('er_prs_f.csv',sep=';')\n",
    "print(er_prs_f.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0aa9ae33",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   pfs_act_nat             label\n",
      "0           26  Kinesitherapeute\n",
      "1           50        Pharmacien\n"
     ]
    }
   ],
   "source": [
    "# Table contenant des informations sur les professionnels de santé\n",
    "ir_act_v=pd.read_csv('ir_act_v.csv', sep=';')\n",
    "print(ir_act_v.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "19969706",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   pfs_spe_cod                label\n",
      "0            1  Medecin generaliste\n",
      "1            6           Radiologue\n"
     ]
    }
   ],
   "source": [
    "# Table contenant des informations sur les professionnels de santé\n",
    "ir_spe_v=pd.read_csv('ir_spe_v.csv', sep=';')\n",
    "print(ir_spe_v.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9b0561d7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   NUM_ENQ  ben_sex_cod  ben_nai_ann  ben_nai_moi  \\\n",
      "0  DPXX:00000000000000001X            2         1963           12   \n",
      "1    DPXX:000000000000002X            1         1971            2   \n",
      "2    DPXX:000000000000003X            1         1962           12   \n",
      "3    DPXX:000000000000004X            2         1959            3   \n",
      "4    DPXX:000000000000005X            1         1998            4   \n",
      "\n",
      "   ben_res_dpt  ben_res_reg  \n",
      "0           75          114  \n",
      "1           93          114  \n",
      "2           93          114  \n",
      "3           94          114  \n",
      "4           93          114  \n"
     ]
    }
   ],
   "source": [
    "#Table contenant les informations des assurés\n",
    "ir_ben_r=pd.read_csv('ir_ben_r.csv', sep=';')\n",
    "print(ir_ben_r.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d99d9d36",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     eta_num                        soc_rai\n",
      "0  750300360  l'Hopital Prive des Peupliers\n",
      "1  750023772            Pharmacie Plaisance\n"
     ]
    }
   ],
   "source": [
    "#Table contenant des informations sur les établissements de santé\n",
    "t_mcoaae=pd.read_csv('t_mcoaae.csv', sep=';')\n",
    "print(t_mcoaae.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2bf7343f",
   "metadata": {},
   "source": [
    "# Creation de la table Person(Python)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6731b04b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Lors de l'importation des donnees de la colonne NUM_ENQ des espaces invisibles ont introduits des erreurs du coup je les enlever \n",
    "ir_ben_r['NUM_ENQ'] = ir_ben_r['NUM_ENQ'].str.strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1174f8ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filtration des données pour Camille Honette (code NUM_ENQ = DPXX:00000000000000001X)\n",
    "person_data = ir_ben_r[ir_ben_r['NUM_ENQ'] == 'DPXX:00000000000000001X']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e1c132be",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Table Person créée et sauvegardée\n"
     ]
    }
   ],
   "source": [
    "if person_data.empty:\n",
    "    print(\"Aucune donnée trouvée pour NUM_ENQ = 'DPXX:00000000000000001X'\")\n",
    "else:\n",
    "    # Création de la table Person\n",
    "    person = pd.DataFrame({\n",
    "        'person_id': [1],  # génère arbitrairement ici\n",
    "        #si ben_sex_cod=1 c'est un homme sin une femme(selon les conventions OMOP)\n",
    "        'gender_concept_id': [8507 if person_data['ben_sex_cod'].values[0] == 1 else 8532], # Concept OMOP pour Male/female\n",
    "        'year_of_birth': person_data['ben_nai_ann'].values, #La méthode .values récupère les valeurs de la série\n",
    "        'month_of_birth': person_data['ben_nai_moi'].values,\n",
    "        'person_source_value': person_data['NUM_ENQ'].values,\n",
    "        'localisation_id': [person_data['ben_res_dpt'].values[0]],  # Utilisation du département de résidence comme location_id\n",
    "        'gender_source_value': person_data['ben_sex_cod'].values\n",
    "    })\n",
    "\n",
    "    # Sauvegarde en fichier CSV\n",
    "    person.to_csv('Person.csv', index=False) # index=False pour éviter d'écrire l'index des lignes dans le fichier\n",
    "    print('Table Person créée et sauvegardée')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abc52c72",
   "metadata": {},
   "source": [
    "# Creation de la table Care Site "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "4f03588c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# importer une interface pour interagir avec des bases de données SQLite à partir de Python\n",
    "import sqlite3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "62b3d8fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Connection à la base de donnees SQLite\n",
    "conn = sqlite3.connect('health_data.db')  \n",
    "#Creation d'un curseur pour executer les requetes SQL et recuperer les resultats\n",
    "cursor = conn.cursor()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "8facf358",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<sqlite3.Cursor at 0x27ee73998c0>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Supprimer la table si elle existe déjà\n",
    "cursor.execute(\"DROP TABLE IF EXISTS Care_Site;\")\n",
    "#Creation de la table\n",
    "create_table_query = \"\"\"\n",
    "CREATE TABLE IF NOT EXISTS Care_Site (\n",
    "    cc_site_id INTEGER PRIMARY KEY,\n",
    "    care_site_name TEXT,\n",
    "    location_id INTEGER,\n",
    "    care_site_source_value TEXT\n",
    ");\n",
    "\"\"\"\n",
    "cursor.execute(create_table_query)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "6970c967",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   cc_site_id               care_site_name  location_id care_site_source_value\n",
      "0           1  Hôpital Privé des Peupliers           75              750300360\n",
      "1           2          Pharmacie Plaisance           75              750023772\n"
     ]
    }
   ],
   "source": [
    "# Insertion des données avec IGNORE pour éviter les doublons\n",
    "# 75 est le code de departement pour Paris\n",
    "# J'ai attribué 1 et 2 arbitrairement comme clé unique\n",
    "# INSERT OR IGNORE signifie que si une entrée avec le même care_site_id existe déjà, l'insertion sera ignorée pour éviter les doublons\n",
    "insert_data_query = \"\"\"\n",
    "INSERT OR IGNORE INTO Care_Site (cc_site_id, care_site_name, location_id, care_site_source_value)\n",
    "VALUES\n",
    "(1, 'Hôpital Privé des Peupliers', 75, '750300360'), \n",
    "(2, 'Pharmacie Plaisance', 75, '750023772');\n",
    "\"\"\"\n",
    "# Execution de la requete\n",
    "cursor.execute(insert_data_query)\n",
    "# Validation des changements\n",
    "conn.commit()\n",
    "# Verification que les donnees ont etes inserees\n",
    "df = pd.read_sql_query(\"SELECT * FROM Care_Site;\", conn)\n",
    "print(df)\n",
    "#Fermeture de la connexion\n",
    "conn.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "867ed9a7",
   "metadata": {},
   "source": [
    "# Creation de la table Provider"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "d4005dea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pyspark in c:\\users\\user\\miniconda3\\lib\\site-packages (3.5.3)\n",
      "Requirement already satisfied: py4j==0.10.9.7 in c:\\users\\user\\miniconda3\\lib\\site-packages (from pyspark) (0.10.9.7)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\user\\miniconda3\\Lib\\site-packages\\pyspark\\sql\\pandas\\conversion.py:485: FutureWarning: is_datetime64tz_dtype is deprecated and will be removed in a future version. Check `isinstance(dtype, pd.DatetimeTZDtype)` instead.\n",
      "  if should_localize and is_datetime64tz_dtype(s.dtype) and s.dt.tz is not None:\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Erreur lors de la sauvegarde en fichier Parquet :  An error occurred while calling o82.parquet.\n",
      ": org.apache.spark.SparkException: Job aborted due to stage failure: Task 3 in stage 0.0 failed 1 times, most recent failure: Lost task 3.0 in stage 0.0 (TID 3) (192.168.0.4 executor driver): org.apache.spark.SparkException: Python worker failed to connect back.\r\n",
      "\tat org.apache.spark.api.python.PythonWorkerFactory.createSimpleWorker(PythonWorkerFactory.scala:203)\r\n",
      "\tat org.apache.spark.api.python.PythonWorkerFactory.create(PythonWorkerFactory.scala:109)\r\n",
      "\tat org.apache.spark.SparkEnv.createPythonWorker(SparkEnv.scala:124)\r\n",
      "\tat org.apache.spark.api.python.BasePythonRunner.compute(PythonRunner.scala:174)\r\n",
      "\tat org.apache.spark.api.python.PythonRDD.compute(PythonRDD.scala:67)\r\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)\r\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:331)\r\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)\r\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:331)\r\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)\r\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:331)\r\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)\r\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:331)\r\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)\r\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:331)\r\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)\r\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:331)\r\n",
      "\tat org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:59)\r\n",
      "\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:104)\r\n",
      "\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:54)\r\n",
      "\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)\r\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\r\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\r\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\r\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\r\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\r\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\r\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1144)\r\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:642)\r\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:1570)\r\n",
      "Caused by: java.net.SocketTimeoutException: Accept timed out\r\n",
      "\tat java.base/sun.nio.ch.NioSocketImpl.timedAccept(NioSocketImpl.java:701)\r\n",
      "\tat java.base/sun.nio.ch.NioSocketImpl.accept(NioSocketImpl.java:745)\r\n",
      "\tat java.base/java.net.ServerSocket.implAccept(ServerSocket.java:695)\r\n",
      "\tat java.base/java.net.ServerSocket.platformImplAccept(ServerSocket.java:660)\r\n",
      "\tat java.base/java.net.ServerSocket.implAccept(ServerSocket.java:636)\r\n",
      "\tat java.base/java.net.ServerSocket.implAccept(ServerSocket.java:582)\r\n",
      "\tat java.base/java.net.ServerSocket.accept(ServerSocket.java:541)\r\n",
      "\tat org.apache.spark.api.python.PythonWorkerFactory.createSimpleWorker(PythonWorkerFactory.scala:190)\r\n",
      "\t... 34 more\r\n",
      "\n",
      "Driver stacktrace:\r\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2856)\r\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2792)\r\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2791)\r\n",
      "\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\r\n",
      "\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\r\n",
      "\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\r\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2791)\r\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1247)\r\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1247)\r\n",
      "\tat scala.Option.foreach(Option.scala:407)\r\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1247)\r\n",
      "\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:3060)\r\n",
      "\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2994)\r\n",
      "\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2983)\r\n",
      "\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\r\n",
      "Caused by: org.apache.spark.SparkException: Python worker failed to connect back.\r\n",
      "\tat org.apache.spark.api.python.PythonWorkerFactory.createSimpleWorker(PythonWorkerFactory.scala:203)\r\n",
      "\tat org.apache.spark.api.python.PythonWorkerFactory.create(PythonWorkerFactory.scala:109)\r\n",
      "\tat org.apache.spark.SparkEnv.createPythonWorker(SparkEnv.scala:124)\r\n",
      "\tat org.apache.spark.api.python.BasePythonRunner.compute(PythonRunner.scala:174)\r\n",
      "\tat org.apache.spark.api.python.PythonRDD.compute(PythonRDD.scala:67)\r\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)\r\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:331)\r\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)\r\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:331)\r\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)\r\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:331)\r\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)\r\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:331)\r\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)\r\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:331)\r\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)\r\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:331)\r\n",
      "\tat org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:59)\r\n",
      "\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:104)\r\n",
      "\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:54)\r\n",
      "\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)\r\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\r\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\r\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\r\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\r\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\r\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\r\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1144)\r\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:642)\r\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:1570)\r\n",
      "Caused by: java.net.SocketTimeoutException: Accept timed out\r\n",
      "\tat java.base/sun.nio.ch.NioSocketImpl.timedAccept(NioSocketImpl.java:701)\r\n",
      "\tat java.base/sun.nio.ch.NioSocketImpl.accept(NioSocketImpl.java:745)\r\n",
      "\tat java.base/java.net.ServerSocket.implAccept(ServerSocket.java:695)\r\n",
      "\tat java.base/java.net.ServerSocket.platformImplAccept(ServerSocket.java:660)\r\n",
      "\tat java.base/java.net.ServerSocket.implAccept(ServerSocket.java:636)\r\n",
      "\tat java.base/java.net.ServerSocket.implAccept(ServerSocket.java:582)\r\n",
      "\tat java.base/java.net.ServerSocket.accept(ServerSocket.java:541)\r\n",
      "\tat org.apache.spark.api.python.PythonWorkerFactory.createSimpleWorker(PythonWorkerFactory.scala:190)\r\n",
      "\t... 34 more\r\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Le fichier source pour les spécialités est ir_spe_v.csv et les données sur les professionnels de santé sont dans er_prs_f.csv\n",
    "!pip install --upgrade pyspark\n",
    "# Importation des modules\n",
    "from pyspark.sql import SparkSession #pour travailler avec dataframes\n",
    "from pyspark.sql.functions import expr #pour executer des expressions SQL sur les colonnes de DF\n",
    "# Création de la session Spark avec le nom de l'application OMOP_Provider \n",
    "spark = SparkSession.builder.appName(\"OMOP_Provider\").getOrCreate()\n",
    "# Chargement des données source \n",
    "ir_spe_v = pd.read_csv('ir_spe_v.csv', sep=';')\n",
    "er_prs_f = pd.read_csv('er_prs_f.csv', sep=';')\n",
    "# Conversion en DataFrame pandas en DataFrame Spark\n",
    "spe_df = spark.createDataFrame(ir_spe_v)\n",
    "prs_df = spark.createDataFrame(er_prs_f)\n",
    "# Filtration des professionnels de santé pour Camille Honette (NUM_ENQ = DPXX:00000000000000001X)\n",
    "prs_filtered = prs_df.filter(prs_df['NUM_ENQ'] == 'DPXX:00000000000000001X')\n",
    "# Jointure pour obtenir les spécialités\n",
    "# specialty_source_value dérivé de pse_spe_cod de prs_filtered\n",
    "# provider_source_value dérivé de pfs_spe_cod de spe_df\n",
    "# specialty_concept_id !!\n",
    "# spe_df contient les specialites medicales\n",
    "# toutes les lignes de prs_filtered seront conservees mm si elles n'ont pas de correspondance dans spe_df\n",
    "# on va joindre 2 DF pour associer les professionelles de sante a leur specialite\n",
    "# j'ai utilise left comme type de jointure pour assurer qu'il ne se produit pas aucune perte de donnees\n",
    "provider = prs_filtered.join(spe_df, prs_filtered.pse_spe_cod == spe_df.pfs_spe_cod, 'left') \\\n",
    "# renommer les colonnes apres la jointure\n",
    "    .selectExpr(\"pse_spe_cod as specialty_source_value\", \"pfs_spe_cod as provider_source_value\") \\\n",
    "#Provider_id genere automatiquement avce 'monotonically_increasing_id()'\n",
    "    .withColumn(\"provider_id\", expr(\"monotonically_increasing_id()\"))\n",
    "try:\n",
    "    # sauvegarde dans un fichier parquet(un format de fichier columnar optimisé pour les traitements Big Data)\n",
    "    provider.write.parquet(\"Provider.parquet\")\n",
    "    print(\"Table Provider créée et sauvegardée.\")\n",
    "except Exception as e:\n",
    "    print(\"Erreur lors de la sauvegarde en fichier Parquet : \", e)\n",
    "\n",
    "#Probleme de communication entre saprk le processus python du coup je vais essayé autre méthode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "94b53ef8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pyspark in c:\\users\\user\\miniconda3\\lib\\site-packages (3.5.3)\n",
      "Requirement already satisfied: py4j==0.10.9.7 in c:\\users\\user\\miniconda3\\lib\\site-packages (from pyspark) (0.10.9.7)\n"
     ]
    }
   ],
   "source": [
    "# Installer PySpark si nécessaire\n",
    "!pip install --upgrade pyspark\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import expr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "64af2110",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pour optimiser l’utilisation des ressources disponibles lors du traitement de données localement avec Sparkr\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"OMOP_Provider\") \\\n",
    "    .config(\"spark.executor.memory\", \"4g\") \\\n",
    "    .config(\"spark.driver.memory\", \"4g\") \\\n",
    "    .config(\"spark.network.timeout\", \"600s\") \\\n",
    "    .config(\"spark.sql.execution.arrow.pyspark.enabled\", \"true\") \\\n",
    "    .config(\"spark.sql.shuffle.partitions\", \"8\") \\\n",
    "    .master(\"local[*]\") \\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "8a2e9d07",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Chargement des données source\n",
    "ir_spe_v = pd.read_csv('ir_spe_v.csv', sep=';')\n",
    "er_prs_f = pd.read_csv('er_prs_f.csv', sep=';')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "1f0c0bf0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\user\\miniconda3\\Lib\\site-packages\\pyspark\\sql\\pandas\\serializers.py:224: FutureWarning: is_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, CategoricalDtype) instead\n",
      "  if is_categorical_dtype(series.dtype):\n",
      "C:\\Users\\user\\miniconda3\\Lib\\site-packages\\pyspark\\sql\\pandas\\conversion.py:351: UserWarning: createDataFrame attempted Arrow optimization because 'spark.sql.execution.arrow.pyspark.enabled' is set to true; however, failed by the reason below:\n",
      "  sun.misc.Unsafe or java.nio.DirectByteBuffer.<init>(long, int) not available\n",
      "Attempting non-optimization as 'spark.sql.execution.arrow.pyspark.fallback.enabled' is set to true.\n",
      "  warn(msg)\n",
      "C:\\Users\\user\\miniconda3\\Lib\\site-packages\\pyspark\\sql\\pandas\\conversion.py:485: FutureWarning: is_datetime64tz_dtype is deprecated and will be removed in a future version. Check `isinstance(dtype, pd.DatetimeTZDtype)` instead.\n",
      "  if should_localize and is_datetime64tz_dtype(s.dtype) and s.dt.tz is not None:\n"
     ]
    }
   ],
   "source": [
    "# Conversion en DataFrame Spark\n",
    "spe_df = spark.createDataFrame(ir_spe_v)\n",
    "prs_df = spark.createDataFrame(er_prs_f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "7f88627d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filtration des professionnels de santé pour Camille Honette (NUM_ENQ = DPXX:00000000000000001X)\n",
    "prs_filtered = prs_df.filter(prs_df['NUM_ENQ'] == 'DPXX:00000000000000001X')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "d29607be",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Jointure pour obtenir les spécialités\n",
    "provider = prs_filtered.join(spe_df, prs_filtered.pse_spe_cod == spe_df.pfs_spe_cod, 'left') \\\n",
    "    .selectExpr(\"pse_spe_cod as specialty_source_value\", \"pfs_spe_cod as provider_source_value\") \\\n",
    "    .withColumn(\"provider_id\", expr(\"monotonically_increasing_id()\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "6ff0e66a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Répartir les données en 4 partitions pour mieux gérer les ressources\n",
    "provider = provider.repartition(4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "1954cecb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Erreur lors de la sauvegarde en fichier Parquet :  An error occurred while calling o165.parquet.\n",
      ": org.apache.spark.SparkException: Job aborted due to stage failure: Task 13 in stage 2.0 failed 1 times, most recent failure: Lost task 13.0 in stage 2.0 (TID 30) (192.168.0.4 executor driver): org.apache.spark.SparkException: Python worker failed to connect back.\r\n",
      "\tat org.apache.spark.api.python.PythonWorkerFactory.createSimpleWorker(PythonWorkerFactory.scala:203)\r\n",
      "\tat org.apache.spark.api.python.PythonWorkerFactory.create(PythonWorkerFactory.scala:109)\r\n",
      "\tat org.apache.spark.SparkEnv.createPythonWorker(SparkEnv.scala:124)\r\n",
      "\tat org.apache.spark.api.python.BasePythonRunner.compute(PythonRunner.scala:174)\r\n",
      "\tat org.apache.spark.api.python.PythonRDD.compute(PythonRDD.scala:67)\r\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)\r\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:331)\r\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)\r\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:331)\r\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)\r\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:331)\r\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)\r\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:331)\r\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)\r\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:331)\r\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)\r\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:331)\r\n",
      "\tat org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:59)\r\n",
      "\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:104)\r\n",
      "\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:54)\r\n",
      "\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)\r\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\r\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\r\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\r\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\r\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\r\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\r\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1144)\r\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:642)\r\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:1570)\r\n",
      "Caused by: java.net.SocketTimeoutException: Accept timed out\r\n",
      "\tat java.base/sun.nio.ch.NioSocketImpl.timedAccept(NioSocketImpl.java:701)\r\n",
      "\tat java.base/sun.nio.ch.NioSocketImpl.accept(NioSocketImpl.java:745)\r\n",
      "\tat java.base/java.net.ServerSocket.implAccept(ServerSocket.java:695)\r\n",
      "\tat java.base/java.net.ServerSocket.platformImplAccept(ServerSocket.java:660)\r\n",
      "\tat java.base/java.net.ServerSocket.implAccept(ServerSocket.java:636)\r\n",
      "\tat java.base/java.net.ServerSocket.implAccept(ServerSocket.java:582)\r\n",
      "\tat java.base/java.net.ServerSocket.accept(ServerSocket.java:541)\r\n",
      "\tat org.apache.spark.api.python.PythonWorkerFactory.createSimpleWorker(PythonWorkerFactory.scala:190)\r\n",
      "\t... 34 more\r\n",
      "\n",
      "Driver stacktrace:\r\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2856)\r\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2792)\r\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2791)\r\n",
      "\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\r\n",
      "\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\r\n",
      "\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\r\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2791)\r\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1247)\r\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1247)\r\n",
      "\tat scala.Option.foreach(Option.scala:407)\r\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1247)\r\n",
      "\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:3060)\r\n",
      "\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2994)\r\n",
      "\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2983)\r\n",
      "\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\r\n",
      "Caused by: org.apache.spark.SparkException: Python worker failed to connect back.\r\n",
      "\tat org.apache.spark.api.python.PythonWorkerFactory.createSimpleWorker(PythonWorkerFactory.scala:203)\r\n",
      "\tat org.apache.spark.api.python.PythonWorkerFactory.create(PythonWorkerFactory.scala:109)\r\n",
      "\tat org.apache.spark.SparkEnv.createPythonWorker(SparkEnv.scala:124)\r\n",
      "\tat org.apache.spark.api.python.BasePythonRunner.compute(PythonRunner.scala:174)\r\n",
      "\tat org.apache.spark.api.python.PythonRDD.compute(PythonRDD.scala:67)\r\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)\r\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:331)\r\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)\r\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:331)\r\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)\r\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:331)\r\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)\r\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:331)\r\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)\r\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:331)\r\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)\r\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:331)\r\n",
      "\tat org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:59)\r\n",
      "\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:104)\r\n",
      "\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:54)\r\n",
      "\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)\r\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\r\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\r\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\r\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\r\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\r\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\r\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1144)\r\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:642)\r\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:1570)\r\n",
      "Caused by: java.net.SocketTimeoutException: Accept timed out\r\n",
      "\tat java.base/sun.nio.ch.NioSocketImpl.timedAccept(NioSocketImpl.java:701)\r\n",
      "\tat java.base/sun.nio.ch.NioSocketImpl.accept(NioSocketImpl.java:745)\r\n",
      "\tat java.base/java.net.ServerSocket.implAccept(ServerSocket.java:695)\r\n",
      "\tat java.base/java.net.ServerSocket.platformImplAccept(ServerSocket.java:660)\r\n",
      "\tat java.base/java.net.ServerSocket.implAccept(ServerSocket.java:636)\r\n",
      "\tat java.base/java.net.ServerSocket.implAccept(ServerSocket.java:582)\r\n",
      "\tat java.base/java.net.ServerSocket.accept(ServerSocket.java:541)\r\n",
      "\tat org.apache.spark.api.python.PythonWorkerFactory.createSimpleWorker(PythonWorkerFactory.scala:190)\r\n",
      "\t... 34 more\r\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Sauvegarde du résultat au format Parquet avec gestion des erreurs\n",
    "try:\n",
    "    provider.write.mode(\"overwrite\").parquet(\"Provider.parquet\")\n",
    "    print(\"Table Provider créée et sauvegardée avec succès.\")\n",
    "except Exception as e:\n",
    "    print(\"Erreur lors de la sauvegarde en fichier Parquet : \", e)\n",
    "\n",
    "# Meme erreur rencontre j'espere que l'ont discuttent pour que je puisse comprendre mieux le probleme\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
